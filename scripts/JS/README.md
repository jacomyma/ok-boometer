## How to use

### Get old tweets
We assume old tweets have been collected using *[Get Old Tweets](https://github.com/Jefferson-Henrique/GetOldTweets-python)* using a query of this kind:
```
python Exporter.py --since 2019-12-17 --until 2019-12-18 --querysearch "ok boomer"
```
You may get one or more files, because the process is unstable and you may have to retry several times and progress time range after time range. It's fine. All the files must be stored in ```/data/got/```.


### Set up the Node environment

The scripts have dependencies. Install them first from the JS folder.

```
npm install
```


### Set up the Twitter API config

Copy ```config.example.js``` into ```config.js```, and edit it to fill your Twitter API credentials. You may also change the function used to filter the "OK Boomer" tweets. You will need your own access to the [Twitter API](https://apps.twitter.com/app/new), there are plenty of tutorials on how to do this online.


### 1. Get an ID list from the Get Old Tweets list of scraped tweets

The tweets harvested by *Get Old Tweets* are not all proper OK Booming tweets, and we do not know if they were in response to another tweet or just someone saying "OK Boomer", or if they just talk about OK Boomer for whatever reason. But we have the ```id``` of the tweet so we can query the Twitter API to look at it and decide if it is an OK-booming. But first we need to extract the list of ids. This script is already discarding obviously non-OK-booming tweets, but it remains too inclusive (it favors false positives over fals negatives). The reason is that Get Old Tweets provides some content, but as it is scraped from the web, it is quite dirty and we cannot take accurate decision on that basis.

Run this script:

```
node got_to_id_list.js
```

It generates two files in ```scripts/data```:
* ```got_id_list.csv``` is just the list of ids, and is used by other scripts.
* ```got_rejected_log.txt``` is a log of the rejected tweets, for monitoring purpose.


### 2. Get metadata from the list if tweet IDs by querying the Twitter API

This script queries Twitter to get the data from all the tweets, then decides, for each tweet, if it is a proper OK-booming or not, and then generates the file used by the front-end (```app/data/okbooming.csv```). However there are far too many tweet ids to harvest for just one query, and Twitter limits how many tweets you can retrieve over time. For that reason, it does as many queries as possible and saves them in a file (```scripts/data/ok-booming-state.csv```). If it crashes before all queries are done, wait 15 minutes (Twitter API's cooldown time) and run the script again.

In practice, the script works as such. First it looks if the state file is present and loads it (```scripts/data/ok-booming-state.csv```). This file lists which tweets have been queried already, or not. Then it looks at other sources of ids to retrieve:
* ```scripts/data/got_id_list.csv``` (tweets to retrieve from Get Old Tweets harvests);
* any files in the ```scripts/data/stream/``` folder, which are files generated by the live stream script.

After loading these files, it parses them and knows exactly which tweets need to be retrieved. Some tweet ids have already been retrieved and are stored in the state file (```scripts/data/ok-booming-state.csv```), and some are not. It creates a stack of tweets to retrieve, queries the API until Twitter blocks the queries, updating the state file every time. When all necessary tweet ids are retrieved, it decides which tweets are actual OK-booming and saves them in a single output file, ```app/data/ok-booming.csv```.

To run this script, you need the Twitter API config. Then just run it this way:

```
node harvest_id_list.js
```

It queries the API and generates the ```okbooming.csv``` file in the ```app/data/``` folder. This file contains which user has OK-Boomed whom and when, including the tweet ids of both the OK-Booming and the OK-Boomed. If the Twitter API has reached its limit, wait 15 minutes and run the script again, until all necessary tweets are retrieved.

**Note 1**: the ```app/data/ok-booming.csv``` file is not directly used by the front-end, because it is too big. To alleviate the load, another script parses that file and generates different views.

**Note 2**: this script is not the only one to write in the ```app/data/ok-booming.csv``` file. There is also ```stream.js``` (see below).


### 3. Compute views

This script computes the front-end views per time modes (year, month...) from okboomings.

If parses the file ```app/data/ok-booming.csv``` and generates a series of files in ```app/data/```: one file for each view (boomed tweets, boomed users...) and each time range (hour, day, week, month, all time). It also generates "table" files to avoind loading redundant info: ```usernameIndex.csv``` and ```whoTweetedIndex.csv``` (self-explanatory).

The rationale here is to avoid querying a complex database and instead having a few static files to load.

Run with:
```
node compute_views.js
```

This script needs to be executed again if the file ```ok-booming.csv``` is updated. In practice, it is sufficient to run it just once every few minutes.

**Note**: in prod we use ```forever``` to run a similar script called ```recurrent_compute_views.js```. The only difference is that that one runs the process every 5 minutes and auto-dies after a while. Then ```forever``` will spawn it again.


### 4. Listen to the Stream API to update the file live

This script listens to the Twitter API and update the data. It listens to the live stream for any tweet containing 'OK boomer' and decides if it is an OK-booming or not. Every time a new OK-booming is detected, it updates the data two ways:
* It updates the prod file ```ok-boomings.csv``` so that next time ```compute_views.js``` (or its recurrent version) is executed, it takes the last ok-boomings in account
* It updates a daily file in the ```scripts/data/stream/``` folder, so that the data can be later reconstructed by ```harvest_id_list.js``` if necessary.

Run with:
```
node stream.js
```

**Note**: this script auto-dies after a while, because in production ```forever``` will reboot it anyway. It ensure that the script does not get stuck in weird places for unknown reasons for too long.


### Monitor your API use

It may help to run the following script, telling you how many calls you have left (regenerates every 15 minutes).

```
node monitor.js
```
